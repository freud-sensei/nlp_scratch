{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec 개선"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CBOW를 어떻게 개선할 수 있을까요?\n",
    "\n",
    "* 굳이 입력층 matmul 계층을 구현할 필요 없다, 가중치행렬의 특정 행벡터만 잘 뽑으면 그만.\n",
    "* 은닉층 이후 계산은, 정답 label에 해당하는 친구에만 집중하면 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 7 8]\n",
      "[15 16 17]\n"
     ]
    }
   ],
   "source": [
    "# embedding 계층 구현\n",
    "import numpy as np\n",
    "W = np.arange(21).reshape(7, 3)\n",
    "print(W[2])\n",
    "print(W[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3,  4,  5],\n",
       "       [ 0,  1,  2],\n",
       "       [ 9, 10, 11],\n",
       "       [ 0,  1,  2]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = np.array([1, 0, 3, 0])\n",
    "W[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "역전파: 앞 층에서 전해진 기울기를, 특정 행에만 더해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.idx = None\n",
    "        \n",
    "    def forward(self, idx):\n",
    "        W, = self.params\n",
    "        self.idx = idx\n",
    "        out = W[idx]\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dW, = self.grads\n",
    "        dW[...] = 0\n",
    "        np.add.at(dW, self.idx, dout)\n",
    "        # dout을 dW의 self.idx번째 행에 더해 준다. for 문보다 빠르다.\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## negative sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 원래: 'you'와 'goodbye'를 줄 때, 타깃 단어는 무엇인가요? --> 다중분류\n",
    "* 변경 이후: 'you'와 'goodbye'를 줄 때, 타깃 단어는 'say(실제 정답)' 인가요? --> 이진분류\n",
    "* 이 경우 softmax function이 아닌 sigmoid function을 사용한다. (사실 softmax를 2개 class 가지고 하는 것과 차이가 없다.)\n",
    "* Embedding Dot layer: 은닉층 뉴런과, 출력 측 가중치에서 정답에 해당하는 단어 벡터 간 내적 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 지금까지는 긍정적인 예(정답)에 대해서만 학습 (정답 레이블은 항상 1)\n",
    "* 부정적인 예(오답)를 입력할 때 결과가 확실하지 않음\n",
    "* 해결방법-네거티브 샘플링: 적은 수의 부정적 예를 샘플링하고, 긍정적 + 부정적 예의 손실을 더한 값을 최종 손실로 함\n",
    "* 말뭉치의 통계 데이터 기초로 샘플링 (자주 등장하는 단어 위주로)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# 확률분포를 기초로 한 샘플링\n",
    "import numpy as np\n",
    "\n",
    "# 0부터 9까지 숫자 중 하나를 샘플링\n",
    "print(np.random.choice(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'참외'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 리스트에서 하나만 샘플링\n",
    "words = ['딸기', '당근', '수박', '참외', '메론']\n",
    "np.random.choice(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['수박' '수박' '메론' '참외' '참외']\n",
      "['수박' '참외' '당근' '메론' '딸기']\n"
     ]
    }
   ],
   "source": [
    "# 여러 개 샘플링\n",
    "print(np.random.choice(words, size=5, replace=True)) # 중복있음\n",
    "print(np.random.choice(words, size=5, replace=False)) # 중복없음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'참외'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 확률분포에 따라 샘플링\n",
    "p = [.5, .1, .05, .2, .15]\n",
    "np.random.choice(words, p=p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.64196878, 0.33150408, 0.02652714])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word2vec에선 기존 확률분포애 .75를 제곱: 확률이 낮은 단어의 확률을 다소 높임\n",
    "p = [.7, .29, .01]\n",
    "new_p = np.power(p, .75)\n",
    "new_p /= np.sum(new_p)\n",
    "new_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4 2]\n",
      " [1 4]\n",
      " [1 3]]\n"
     ]
    }
   ],
   "source": [
    "# UnigramSampler 클래스를 이용\n",
    "from negative_sampling_layer import UnigramSampler\n",
    "corpus = np.array([0, 1, 2, 3, 4, 1, 2, 3])\n",
    "power = .75\n",
    "sample_size = 2\n",
    "\n",
    "sampler = UnigramSampler(corpus, power, sample_size)\n",
    "target = np.array([1, 3, 0])\n",
    "negative_sample = sampler.get_negative_sample(target)\n",
    "print(negative_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`negative_sampling_layer.py`를 보세요!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 개선된 word2vec 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`cbow.py`, `train.py`, `eval.py`를 보세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* word2vec에서 구한 단어의 분산 표현은, 단어의 복잡한 패턴을 파악할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 기타 주제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 전이 학습: 기존 단어의 분산 표현을 이용해 자연어 처리 과제를 수행한다.\n",
    "  * 단어와 문장을 고정길이 벡터로 변환할 수 있다. -> RNN 등 신경망에 적용할 수 있다.\n",
    "\n",
    "* 분산 표현이 좋은지 평가할 수 있는 방법?\n",
    "  * 단어의 유사성, 유추 문제를 활용해 평가"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
