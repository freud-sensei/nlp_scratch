{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 어텐션"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## encoder 개선\n",
    "* seq2seq은 encoder의 입력 문장과 상관없이, 고정 길이 벡터를 decoder로 전달한다\n",
    "* 입력 문장이 길수록 손실이 발생한다 -> 출력의 길이를 입력 문장의 길이에 따라 바꾸어야 한다\n",
    "* 해결법: 각 시각의 은닉 상태 벡터를 모두 이용한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## decoder 개선\n",
    "* 목표: 도착어 단어와 대응하는 출발어 단어의 정보를 골라내기 -> 어텐션\n",
    "  * 예: Decoder가 \"I\"를 출력할 때, hs에서 \"나\"에 대응하는 벡터를 선택해야 함\n",
    "* LSTM과 Affine계층 사이 새로운 계층 추가\n",
    "  * 입력은 2가지: Encoder로부터 받는 hs 행렬 + 시각별 LSTM 계층의 은닉상태\n",
    "  * 각 단어의 가중치를 별도로 계산 후, hs의 행벡터에 대한 가중합 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 4)\n",
      "(5, 4)\n",
      "(4,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "T, H = 5, 4 # 시계열의 길이, 은닉상태 원소 수\n",
    "hs = np.random.randn(T, H)\n",
    "a = np.array([.8, .1, .03, .05, .02])\n",
    "\n",
    "ar = a.reshape(T, 1).repeat(H, axis=1)\n",
    "print(ar.shape)\n",
    "\n",
    "t = hs * ar # 원소별 곱을 계산한다.\n",
    "print(t.shape)\n",
    "\n",
    "c = np.sum(t, axis=0) # 가중합을 계산한다. (axis=0 -> 0번째 축이 사라진다.)\n",
    "print(c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 5, 4)\n",
      "(10, 4)\n"
     ]
    }
   ],
   "source": [
    "# 미니배치 처리용\n",
    "N, T, H = 10, 5, 4\n",
    "hs = np.random.randn(N, T, H)\n",
    "a = np.random.randn(N, T) # sequence별로 가중치는 다르겠지\n",
    "ar = a.reshape(N, T, 1).repeat(H, axis=2)\n",
    "\n",
    "t = hs * ar\n",
    "print(t.shape)\n",
    "\n",
    "c = np.sum(t, axis=1)\n",
    "print(c.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가중합 a 구하는 방법\n",
    "* LSTM의 은닉상태 벡터를 $\\bf{h}$라 할 때, $\\bf{h}$가 $\\bf{hs}$의 단어벡터와 얼마나 비슷한지 파악하기 -> 내적\n",
    "* h와 hs의 각 행벡터 간 내적을 구하고, 소프트맥스 함수로 정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 5)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.layers import Softmax\n",
    "import numpy as np\n",
    "\n",
    "N, T, H = 10, 5, 4\n",
    "hs = np.random.randn(N, T, H)\n",
    "h = np.random.randn(N, H)\n",
    "hr = h.reshape(N, 1, H).repeat(T, axis=1)\n",
    "\n",
    "t = hs * hr\n",
    "s = np.sum(t, axis=2)\n",
    "softmax = Softmax()\n",
    "a = softmax.forward(s)\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "구현은 `attention_layer.py`를 보세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention을 갖춘 seq2seq 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Encoder: **모든 시간**의 은닉계층을 반환해야 한다는 점 외에는 Decoder와 동일하다\n",
    "* Decoder: LSTM과 Affine layer 사이에 Attention Layer가 포함되며, Affine Layer는 LSTM의 은닉상태 벡터와 Attention Layer의 맥락벡터를 concatenate해서 입력한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`attention_seq2seq.py` 를 보세요."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
